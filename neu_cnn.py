# -*- coding: utf-8 -*-
"""NEU_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-U7yn-AvfIj-dMFxq2l8zqrGDqkYxlxC
"""

# import all the required libraries
from pyspark.sql import SparkSession
import tensorflow as tf
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models
from pyspark.ml.feature import VectorAssembler
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import cv2

from google.colab import drive
drive.mount('/content/drive')

# Uploading zip file to preserve the data structure
from google.colab import files
files.upload()

# Unzip the data folder
!unzip -q "archive (1).zip" -d /content/dataset
!find /content/dataset -maxdepth 2 -type d | head -n 50

import os
import pandas as pd

 # build the path to the images folder inside the given split directory
def build_df(base_path, split):
    records = []
    image_root = os.path.join(base_path, 'images')

    # loop over each subfolder inside images/ (each subfolder name is treated as a label/class)
    for label in os.listdir(image_root):
        label_path = os.path.join(image_root, label)

        # skip anything that is not a folder (safety check)
        if not os.path.isdir(label_path):
            continue

        # loop over each file inside the label folder
        for img in os.listdir(label_path):

            # keep only image files with allowed extensions
            if img.lower().endswith(('.jpg', '.png', '.jpeg')):

              # add a record (row) describing this image
                records.append({
                    'image': img, # image filename (e.g., crazing_268.jpg)
                    'label': label, # class label = folder name (e.g., crazing)
                    'label_dir': label, # folder name used later to build full filepath
                    'split': split  # which dataset split this belongs to (train/validation)
                })

    # convert the list of records into dataframe
    return pd.DataFrame(records)

# build DataFrame for training split by scanning train/images/<label>/
train_df = build_df('/content/dataset/NEU-DET/train', 'train')

# build DataFrame for validation split by scanning train/images/<label>/
val_df   = build_df('/content/dataset/NEU-DET/validation', 'validation')

# Print number of rows and columns
print(train_df.shape, val_df.shape)

from sklearn.model_selection import train_test_split

# Assigning unique labels
label2id = {lbl: i for i, lbl in enumerate(sorted(train_df['label'].unique()))}
id2label = {v: k for k, v in label2id.items()}

# Assigning label encoding to train and validation set
train_df['label_id'] = train_df['label'].map(label2id)
val_df['label_id'] = val_df['label'].map(label2id)

#Split the data to train_df into train_df2 and test_df
train_df2, test_df = train_test_split(
    train_df,
    test_size=0.15,
    random_state=42,
    stratify=train_df['label_id']
)

import os

TRAIN_BASE = "/content/dataset/NEU-DET/train"
VAL_BASE   = "/content/dataset/NEU-DET/validation"

#Creating filepath for Train, val and test dataframes
train_df2["filepath"] = train_df2.apply(
    lambda r: os.path.join(TRAIN_BASE, "images", r["label_dir"], r["image"]), axis=1
)
val_df["filepath"] = val_df.apply(
    lambda r: os.path.join(VAL_BASE, "images", r["label_dir"], r["image"]), axis=1
)
test_df["filepath"] = test_df.apply(
    lambda r: os.path.join(TRAIN_BASE, "images", r["label_dir"], r["image"]), axis=1
)

print(train_df2.shape, val_df.shape, test_df.shape)

import matplotlib.pyplot as plt

for name, df in [("Train", train_df2), ("Val", val_df), ("Test", test_df)]:
    counts = df["label"].value_counts().sort_index()

    plt.figure(figsize=(8, 4))
    plt.plot(counts.index, counts.values, marker="o")  # line plot
    plt.title(f"{name} class counts")
    plt.xticks(rotation=45)
    plt.ylabel("count")
    plt.xlabel("label")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# Used cv2 t visualize the data , check for corrupted images, & weird color issues
#import cv2
#import matplotlib.pyplot as plt

#def load_image_cv2(path, image_size=(225, 225)):
 #   img = cv2.imread(path)
  #  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
   # img = cv2.resize(img, image_size)
    #return img

# sample = train_df2.sample(9, random_state=42).reset_index(drop=True)

#plt.figure(figsize=(9, 9))
#for i in range(9):
    #img = load_image_cv2(sample.loc[i, "filepath"], (225, 225))
    #plt.subplot(3, 3, i+1)
    #plt.imshow(img)
    #plt.title(sample.loc[i, "label"])
    #plt.axis("off")
#plt.tight_layout()
#plt.show()

# Using tensor flow to validate training data for decode, resize, normalization &
# batching and shuffling are correct

import tensorflow as tf

#Setting the image size and trying batch size =16
IMG_SIZE = (224, 224)
BATCH = 16

# function to write file paths and label_id's
def ds_function(df, shuffle=False):
    paths = df["filepath"].values
    labels = df["label_id"].values

  # slicing the paths and labels
    ds = tf.data.Dataset.from_tensor_slices((paths, labels))

    def tf_loads(path, label):
        img = tf.io.read_file(path)
        img = tf.image.decode_jpeg(img, channels=3)  # works for jpg/jpeg; png also usually OK
        img = tf.image.resize(img, IMG_SIZE)
        img = tf.cast(img, tf.float32) / 255.0
        return img, label

  # Converting image & path into tensor_img & tensor_path
    ds = ds.map(tf_loads, num_parallel_calls=tf.data.AUTOTUNE)

    # Randomize the training set
    if shuffle:
        ds = ds.shuffle(buffer_size=min(len(df), 1000), seed=42)
    ds = ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)     #Groups samples into batches
    return ds

train_ds = ds_function(train_df2, shuffle=True)
val_ds   = ds_function(val_df, shuffle=False)
test_ds  = ds_function(test_df, shuffle=False)

# Peek at one batch shape
x, y = next(iter(train_ds))
print("Batch image shape:", x.shape, "Batch labels shape:", y.shape)

#Visualizing the images
images, labels = next(iter(train_ds))

plt.figure(figsize=(9, 9))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(images[i].numpy())
    plt.title(id2label[int(labels[i].numpy())])
    plt.axis("off")
plt.tight_layout()
plt.show()

import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet import preprocess_input

num_classes = len(label2id)

base = ResNet50(include_top=False, weights="imagenet", input_shape=IMG_SIZE+(3,))
base.trainable = False

aug = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.05),
    layers.RandomZoom(0.1),
    layers.RandomContrast(0.1),
], name="aug")

inputs = layers.Input(shape=IMG_SIZE+(3,))
x = aug(inputs)
x = preprocess_input(x)
x = base(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = Model(inputs, outputs)
loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)

# Train the “new head” first (feature extractor mode)
# The model learns a good classifier head for your classes quickly
# Head Training

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',  # Changed from categorical
    metrics=['accuracy']
)

cbs = [
    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.2),
]
history = model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=cbs)

# Fine Tune part of the base(adapt features to your dataset)
# Once the head is decent, you can improve accuracy by letting the model adjust some pretrained layers to your dataset (domain shift).
# Freezing base.layers[:-30] keeps most layers stable and only fine-tunes the last ~30 layers
base.trainable = True
for layer in base.layers[:-30]:  # tune last 30 layers only
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),  # IMPORTANT: small LR
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=["accuracy"]
)

cbs = [
    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.2),
]

history2 = model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=cbs)

import matplotlib.pyplot as plt

def plot_two_phase(history1, history2, title_prefix="Two-Phase"):
    h1, h2 = history1.history, history2.history

    # combine lists
    train_acc = h1.get("accuracy", []) + h2.get("accuracy", [])
    val_acc   = h1.get("val_accuracy", []) + h2.get("val_accuracy", [])
    train_loss = h1.get("loss", []) + h2.get("loss", [])
    val_loss   = h1.get("val_loss", []) + h2.get("val_loss", [])

    e1 = len(h1.get("loss", []))
    e2 = len(h2.get("loss", []))
    epochs = range(1, e1 + e2 + 1)

    # Accuracy
    plt.figure(figsize=(8, 4))
    if len(train_acc) > 0: plt.plot(epochs, train_acc, label="train_accuracy")
    if len(val_acc) > 0:   plt.plot(epochs, val_acc, label="val_accuracy")
    if e1 > 0:             plt.axvline(e1 + 0.5, linestyle="--", label="phase boundary")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title(f"{title_prefix} Accuracy (Phase1 + Phase2)")
    plt.legend()
    plt.show()

    # Loss
    plt.figure(figsize=(8, 4))
    if len(train_loss) > 0: plt.plot(epochs, train_loss, label="train_loss")
    if len(val_loss) > 0:   plt.plot(epochs, val_loss, label="val_loss")
    if e1 > 0:              plt.axvline(e1 + 0.5, linestyle="--", label="phase boundary")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"{title_prefix} Loss (Phase1 + Phase2)")
    plt.legend()
    plt.show()

# usage
plot_two_phase(history, history2, "Transfer Learning")

!git init
!git add .
!git commit -m "Initial commit"
!git remote add origin https://github.com/laharika1/NEU_CNN.git
!git branch -M main
!git push -u origin main

# Commented out IPython magic to ensure Python compatibility.
# Go to content folder
# %cd /content

# Create a clean project folder
!mkdir -p NEU_CNN

# Copy your project files into it (adjust if needed)
!cp -r NEU-DET NEU_CNN/ 2>/dev/null || true
!cp *.ipynb NEU_CNN/ 2>/dev/null || true
!cp *.py NEU_CNN/ 2>/dev/null || true

# Move into project folder
# %cd NEU_CNN

# Create .gitignore (to avoid pushing large data)
!echo "sample_data/" > .gitignore
!echo "*.zip" >> .gitignore
!echo "__pycache__/" >> .gitignore
!echo "*.pyc" >> .gitignore

# Initialize git
!git init

# Set your GitHub identity
!git config --global user.name "Laharika Mamidi"
!git config --global user.email "YOUR_GITHUB_EMAIL"

# Add and commit
!git add .
!git commit -m "Initial commit: NEU_CNN"

# Connect to GitHub repo
!git branch -M main
!git remote add origin https://github.com/laharika1/NEU_CNN

# Push to GitHub
!git push -u origin main

# Commented out IPython magic to ensure Python compatibility.
# -----------------------------
# STEP 1: Go to content folder
# -----------------------------
# %cd /content

# -----------------------------
# STEP 2: Create clean project folder
# -----------------------------
!rm -rf NEU_CNN
!mkdir NEU_CNN

# Copy your project files (adjust if needed)
!cp -r NEU-DET NEU_CNN/ 2>/dev/null || true
!cp *.ipynb NEU_CNN/ 2>/dev/null || true
!cp *.py NEU_CNN/ 2>/dev/null || true

# %cd NEU_CNN

# -----------------------------
# STEP 3: Create .gitignore
# -----------------------------
!echo "sample_data/" > .gitignore
!echo "*.zip" >> .gitignore
!echo "__pycache__/" >> .gitignore
!echo "*.pyc" >> .gitignore
!echo "dataset/" >> .gitignore

# -----------------------------
# STEP 4: Initialize Git
# -----------------------------
!git init
!git config --global user.name "Laharika Mamidi"
!git config --global user.email "YOUR_GITHUB_EMAIL"

# -----------------------------
# STEP 5: Add and Commit
# -----------------------------
!git add .
!git commit -m "Initial commit: NEU_CNN project"

# -----------------------------
# STEP 6: Connect to GitHub
# -----------------------------
!git branch -M main
!git remote remove origin 2>/dev/null || true
!git remote add origin https://github.com/laharika1/NEU_CNN.git

# -----------------------------
# STEP 7: Secure Push Using Token
# -----------------------------
import getpass

token = getpass.getpass("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")

!git remote set-url origin https://laharika1:{token}@github.com/laharika1/NEU_CNN.git
!git push -u origin main

